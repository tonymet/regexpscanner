<!-- Code generated by gomarkdoc. DO NOT EDIT -->

# regexpscanner

```go
import "github.com/tonymet/regexpscanner"
```

©️ 2024 Anthony Metzidis

regexpscanner \-\- stream\-based scanner and regex\-based tokenizer in one.

scans io.Reader streams and returns matching tokens

## Index

- [func MakeScanner\(in io.Reader, re \*regexp.Regexp\) \*bufio.Scanner](<#MakeScanner>)
- [func MakeSplitter\(re \*regexp.Regexp\) func\(\[\]byte, bool\) \(int, \[\]byte, error\)](<#MakeSplitter>)
- [func ProcessTokens\(in io.Reader, re \*regexp.Regexp, handler func\(string\)\)](<#ProcessTokens>)


<a name="MakeScanner"></a>
## func [MakeScanner](<https://github.com/tonymet/regexpscanner/blob/main/regexpscanner.go#L37>)

```go
func MakeScanner(in io.Reader, re *regexp.Regexp) *bufio.Scanner
```

MakeScanner creates a scanner you can call scanner.Scan\(\) and scanner.Text\(\) with.

Calling scanner.Scan\(\) && scanner.Text\(\) will return the latest token matching the regex in the stream.

<details><summary>Example</summary>
<p>

use MakeScanner to create a scanner that will tokenize using the regex

```go
package main

import (
	"fmt"
	"regexp"
	"strings"

	rs "github.com/tonymet/regexpscanner"
)

func main() {
	scanner := rs.MakeScanner(strings.NewReader("<html><body><p>Welcome to My Website</p></body></html>"),
		regexp.MustCompile(`</?[a-z]+>`),
	)
	// scanner has Split function defined using the regexp passed to MakeScanner
	for scanner.Scan() {
		fmt.Println(scanner.Text())
	}
}
```

#### Output

```
<html>
<body>
<p>
</p>
</body>
</html>
```

</p>
</details>

<a name="MakeSplitter"></a>
## func [MakeSplitter](<https://github.com/tonymet/regexpscanner/blob/main/regexpscanner.go#L20>)

```go
func MakeSplitter(re *regexp.Regexp) func([]byte, bool) (int, []byte, error)
```

MakeSplitter\(re\) creates a splitter to be passed to scanners.Split\(\) the re will be used to tokenize input passed to the scanner.

splitters can be wrapped with more complicated splitters for further processing see bufio.Scanner for example splitter\-wrappers

<details><summary>Example</summary>
<p>

use MakeSplitter to create a "splitter" for scanner.Split\(\)

```go
package main

import (
	"bufio"
	"fmt"
	"regexp"
	"strings"

	rs "github.com/tonymet/regexpscanner"
)

func main() {
	splitter := rs.MakeSplitter(regexp.MustCompile(`</?[a-z]+>`))
	scanner := bufio.NewScanner(strings.NewReader("<html><body><p>Welcome to My Website</p></body></html>"))
	// be sure to call Split()
	scanner.Split(splitter)
	for scanner.Scan() {
		fmt.Println(scanner.Text())
	}
}
```

#### Output

```
<html>
<body>
<p>
</p>
</body>
</html>
```

</p>
</details>

<a name="ProcessTokens"></a>
## func [ProcessTokens](<https://github.com/tonymet/regexpscanner/blob/main/regexpscanner.go#L44>)

```go
func ProcessTokens(in io.Reader, re *regexp.Regexp, handler func(string))
```

ProcessTokens calls handler\(string\) for each matching token from the Scanner.

<details><summary>Example</summary>
<p>

use ProcessTokens when a simple callback\-based stream tokenizer is needed

```go
package main

import (
	"fmt"
	"regexp"
	"strings"

	rs "github.com/tonymet/regexpscanner"
)

func main() {
	rs.ProcessTokens(
		strings.NewReader("<html><body><p>Welcome to My Website</p></body></html>"),
		regexp.MustCompile(`</?[a-z]+>`),
		func(text string) {
			fmt.Println(text)
		})
}
```

#### Output

```
<html>
<body>
<p>
</p>
</body>
</html>
```

</p>
</details>

Generated by [gomarkdoc](<https://github.com/princjef/gomarkdoc>)
